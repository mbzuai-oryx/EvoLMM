<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete" lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="description" content="EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards">
  <meta name="keywords" content="EvoLMM, Large Multimodal Models, self-evolving, continuous reward, unsupervised, vision-language">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EvoLMM</title>

  <style type="text/css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;-webkit-box-sizing:border-box;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.3333333333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:solid .08em #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s infinite linear;animation:fa-spin 2s infinite linear}.fa-pulse{-webkit-animation:fa-spin 1s infinite steps(8);animation:fa-spin 1s infinite steps(8)}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{-webkit-transform:scale(1,-1);transform:scale(1,-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1,-1);transform:scale(-1,-1)}:root .fa-flip-both,:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-rotate-90{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2.5em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1.25em}.svg-inline--fa.fa-stack-2x{height:2em;width:2.5em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}.svg-inline--fa .fa-primary{fill:var(--fa-primary-color,currentColor);opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa .fa-secondary{fill:var(--fa-secondary-color,currentColor);opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-primary{opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-secondary{opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa mask .fa-primary,.svg-inline--fa mask .fa-secondary{fill:#000}.fad.fa-inverse{color:#fff}</style>
  <link rel="stylesheet" href="./assets/css">
  <link rel="stylesheet" href="./assets/bulma.min.css">
  <link rel="stylesheet" href="./assets/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.3/css/academicons.min.css">
  <link rel="stylesheet" href="./assets/all.min.css">
  <link rel="stylesheet" href="./assets/index.css">
  <link rel="icon" href="./assets/favicon.png">
  <link href="./assets/icon" rel="stylesheet">

  <script src="./assets/jquery.min.js"></script><style type="text/css" id="operaUserStyle"></style>
  <script defer src="./assets/all.min.js"></script>
  <script type="module" src="./assets/gradio.js"></script>

  <style>
  .section {
    margin-bottom: -30px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  /* gradient mask like original for expandable cards (kept here if needed) */
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  table.data-table {
    width: 100%;
    border-collapse: collapse;
    text-align: center;
    margin: 0 auto;
    font-size: 0.95rem;
    border: 1px solid #ddd;
  }

  table.data-table caption {
    caption-side: top;
    font-weight: 600;
    padding: 10px;
    color: #333;
  }

  table.data-table th,
  table.data-table td {
    border: 1px solid #dcdcdc;
    padding: 8px 10px;
    vertical-align: middle;
  }

  table.data-table thead th {
    background-color: #f5f5f5; /* light gray header */
    font-weight: 700;
    color: #000;
  }

  table.data-table tbody tr:nth-child(even):not(.highlight) {
    background-color: #fafafa; /* subtle alternating rows */
  }

  table.data-table tbody tr:hover {
    background-color: #f0f0f9; /* light hover tone */
  }

  table.data-table tr.highlight {
    background-color: #e6e6ff !important; /* soft blue highlight for EvoLMM */
    font-weight: 600;
  }

  table.data-table tr.highlight td {
    font-weight: 600;
  }

  table.data-table thead {
    border-bottom: 2px solid #ccc; /* thin divider between head/body */
  }

  caption {
    caption-side: top;
    padding: 8px;
    font-weight: 600;
  }

  #BibTeX {
    margin-bottom: -80px;
  }

  #Acknowledgement {
    margin-top: -80px;
  }
</style>

  <script type="module" src="./assets/index-9405f928.js"></script>
</head>

<body>

  <!-- HERO -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              EvoLMM: <span class="is-size-2">Self-Evolving Large Multimodal Models with Continuous Rewards</span>
            </h1>

            <div class="is-size-5 publication-authors">
              <div class="author-group">
                  <span class="author-block">
                      <a href="https://omkarthawakar.github.io/" style="color:#f68946;font-weight:normal;">Omkar Thawakar<sup>*</sup></a>,
                  </span>
                  <span class="author-block">
                      <a href="https://shravfolio.vercel.app" style="color:#008AD7;font-weight:normal;">Shravan Venkatraman<sup>*</sup></a>,
                  </span>
                  <span class="author-block">
                      <a href="https://scholar.google.com/citations?hl=en&user=9-2AnjQAAAAJ&view_op=list_works&sortby=pubdate" style="color:#F2A900;font-weight:normal;">Ritesh Thawkar<sup>*</sup></a>,
                  </span>
              </div>

              <div class="author-group">
                  <span class="author-block">
                      <a href="https://amshaker.github.io/" style="color:#f68946;font-weight:normal;">Abdelrahman M Shaker</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://scholar.google.ae/citations?user=bZ3YBRcAAAAJ&hl=fr" style="color:#f68946;font-weight:normal;">Hisham Cholakkal</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://scholar.google.fi/citations?user=_KlvMVoAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Rao Muhammad Anwer</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://salman-h-khan.github.io/" style="color:#f68946;font-weight:normal;">Salman Khan</a>,
                  </span>
                  <span class="author-block">
                      <a href="https://sites.google.com/view/fahadkhans/home" style="color:#f68946;font-weight:normal;">Fahad Shahbaz Khan</a>
                  </span>
              </div>
            </div>

            <div class="is-size-5 publication-authors">
              Mohamed bin Zayed University of AI (MBZUAI), Australian National University, Aalto University, LinkÃ¶ping University<br>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equally contributing first authors</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="./assets/arxiv.svg" alt="arXiv" style="height:1em; width:1em; filter: brightness(0) invert(1);">
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/mbzuai-oryx/EvoLMM" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8z"></path></svg>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-justified">
          EvoLMM is a <b>fully unsupervised</b> self-evolving framework for LMMs that improves visual reasoning from <b>raw images only</b>, by coupling a <b>Proposer</b> and a <b>Solver</b> trained via <b>continuous self-consistency rewards</b>.
        </h4>
      </div>
    </div>
  </section>

  <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto âˆ¼3% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">ðŸ”¥Highlights</h2>
        <div class="content has-text-justified">
          <ol type="1">
            <li>
              We introduce a self-evolving multimodal framework, named <span style="color:#997300;">EvoLMM</span>, that enables a base LMM to improve without human labels, metadata, or external reward models. The framework decomposes the model into two internal roles, <em>Proposer</em> and <em>Solver</em>, forming a closed-loop propose-solve cycle trained solely through internal consistency feedback.
            </li>
            <br>
            <li>
              We develop a continuous self-rewarding mechanism based on multi-sample answer consistency, which replaces both learned discrete reward models and semantic similarity scoring used in prior LMM self-evolution approaches. This continuous internal reward signal provides smooth gradients and stable optimization, enabling consistent improvement in performance.
            </li>
            <br>
            <li>
              We empirically validate EvoLMM on mathematical visual reasoning benchmarks, with absolute gains of ~2â€“3% over the Qwen-2.5-VL-7B baseline using only raw images during training. We further analyze the evolution of our propose-solve mechanism where the difficulty level gradually progresses and maintains stable learning, showing that the model naturally develops more structured and grounded reasoning behaviors over time. Furthermore, we show that internal consistency can serve as a viable supervision signal for open-ended multimodal learning.
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>



  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">EvoLMM Pipeline</h2>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths" style="display:flex; align-items:flex-start; justify-content:center;">
          <figure style="text-align: center;">
            <img id="teaser" style="width:75%; height:auto;" src="./assets/architecture.png" alt="EvoLMM Pipeline Diagram">
            <figcaption>
              The Proposer generates visually grounded questions from raw images; the Solver answers them multiple times. Self-consistency among answers yields continuous rewards for the Solver, while the Proposer is rewarded for mid-entropy (moderate difficulty) questionsâ€”forming a closed-loop self-evolving curriculum.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>

  <!-- Continuous Reward Explanation -->
  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Continuous Reward: Why it Stabilizes Self-Evolution</h2>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              Early in training, Solver outputs over images are diverse; discrete majority-vote rewards become sparse and unstable. EvoLMM replaces them with a <b>continuous self-consistency</b> signal that scales smoothly with agreement and penalizes verbosity lightly. The Proposer uses an <b>entropy-guided band-pass</b> reward that peaks at moderate difficulty, discouraging trivial or unsolvable questions and inducing an emergent curriculum as the Solver improves. Both roles are optimized via <b>KL-regularized REINFORCE</b> with moving baselines for stability.
              Thus, our proposed continuous self-evolving design provides a non-zero learning signal even when the model is uncertain, avoiding stagnation observed in the discrete-reward self-questioning scheme. Further, our design enables the Proposer to continuously adjust question difficulty to match the Solverâ€™s evolving capability, thereby mitigating model collapse. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- RESULTS: Table 1 -->
  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Main Results</h2>
      </div>
    </div>

    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div style="text-align: center;">
            <table class="data-table" style="width: 100%;">
              <caption>Evaluation results across eight multimodal mathematical and visual reasoning benchmarks.</caption>
              <thead>
                  <tr>
                      <th>Model</th>
                      <th>ChartQA</th>
                      <th>MathVista</th>
                      <th>MathVision</th>
                      <th>MathVerse</th>
                      <th>InfoGraphic-VQA<sub>val</sub></th>
                      <th>AI2D</th>
                      <th>ScienceQA</th>
                      <th>MMMU<sub>val</sub></th>
                  </tr>
              </thead>
              <tbody>
                  <tr>
                      <td>Vision-Zeroâ€  (CLEVR)</td>
                      <td>84.24</td><td>68.43</td><td>23.96</td><td>43.86</td><td>80.35</td><td>82.64</td><td>88.50</td><td>51.44</td>
                  </tr>
                  <tr>
                      <td>Qwen2.5-VL-7B (Baseline)</td>
                      <td>84.00</td><td>68.46</td><td>23.91</td><td>43.78</td><td>80.44</td><td>82.61</td><td>88.30</td><td>51.11</td>
                  </tr>
                  <tr>
                      <td>Qwen2.5-VL-7B + Discrete Reward</td>
                      <td>84.62</td><td>68.88</td><td>22.52</td><td>42.10</td><td>80.52</td><td>82.18</td><td>87.98</td><td>50.84</td>
                  </tr>
                  <tr class="highlight">
                      <td><b>Qwen2.5-VL-7B + Continuous Reward (EvoLMM)</b></td>
                      <td><b>86.70</b></td><td><b>70.52</b></td><td><b>24.81</b></td><td><b>44.88</b></td><td><b>81.06</b></td><td><b>83.41</b></td><td><b>89.50</b></td><td><b>52.01</b></td>
                  </tr>
                  <tr>
                      <td>Î” Improvement</td>
                      <td>+2.70</td><td>+2.06</td><td>+0.90</td><td>+1.10</td><td>+0.62</td><td>+0.80</td><td>+1.20</td><td>+0.90</td>
                  </tr>
              </tbody>
            </table>
            <p class="soft" style="margin-top:6px;">â€  uses external supervision.</p>
          </div>
        </div>
      </div>

      <!-- Table 2 -->
      <br>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div style="text-align: center;">
            <table class="data-table" style="width: 100%;">
              <caption>Comparison of our EvoLMM self-evolving framework under different parameter update strategies.</caption>
              <thead>
                  <tr>
                      <th>Model</th>
                      <th>ChartQA</th>
                      <th>MathVista</th>
                      <th>MathVision</th>
                      <th>MathVerse</th>
                      <th>InfoGraphic-VQA<sub>val</sub></th>
                      <th>AI2D</th>
                      <th>ScienceQA</th>
                      <th>MMMU<sub>val</sub></th>
                  </tr>
              </thead>
              <tbody>
                  <tr>
                      <td>Qwen2.5-VL-7B (Baseline)</td>
                      <td>84.00</td><td>68.46</td><td>23.91</td><td>43.78</td><td>80.44</td><td>82.61</td><td>88.30</td><td>51.11</td>
                  </tr>
                  <tr class="highlight">
                      <td><b>Qwen2.5-VL-7B + LoRA (EvoLMM)</b></td>
                      <td><b>86.70</b></td><td><b>70.52</b></td><td><b>24.81</b></td><td><b>44.88</b></td><td><b>81.06</b></td><td><b>83.41</b></td><td><b>89.50</b></td><td><b>52.01</b></td>
                  </tr>
                  <tr>
                      <td>Qwen2.5-VL-7B + QLoRA</td>
                      <td>85.32</td><td>68.92</td><td>23.97</td><td>43.82</td><td>80.83</td><td>82.75</td><td>88.73</td><td>51.71</td>
                  </tr>
                  <tr>
                      <td>Qwen2.5-VL-7B + Full-Finetune</td>
                      <td>84.20</td><td>68.41</td><td>23.37</td><td>43.77</td><td>80.37</td><td>82.64</td><td>88.12</td><td>51.23</td>
                  </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>

      <!-- Table 3 -->
      <br>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div style="text-align: center;">
            <table class="data-table" style="width: 100%;">
              <caption>Effectiveness of our EvoLMM self-evolving framework across different large multimodal backbones.</caption>
              <thead>
                  <tr>
                      <th>Model</th>
                      <th>ChartQA</th>
                      <th>MathVista</th>
                      <th>MathVision</th>
                      <th>MathVerse</th>
                      <th>InfoGraphic-VQA<sub>val</sub></th>
                      <th>AI2D</th>
                      <th>ScienceQA</th>
                      <th>MMMU<sub>val</sub></th>
                  </tr>
              </thead>
              <tbody>
                  <tr>
                      <td>Qwen2.5-VL-7B (Base)</td>
                      <td>84.00</td><td>68.46</td><td>23.91</td><td>43.78</td><td>80.44</td><td>82.61</td><td>88.30</td><td>51.11</td>
                  </tr>
                  <tr class="highlight">
                      <td><b>Qwen2.5-VL-7B (EvoLMM)</b></td>
                      <td><b>86.70</b></td><td><b>70.52</b></td><td><b>24.81</b></td><td><b>44.88</b></td><td><b>81.06</b></td><td><b>83.41</b></td><td><b>89.50</b></td><td><b>52.01</b></td>
                  </tr>
                  <tr>
                      <td>InternVL3-8B-Instruct (Base)</td>
                      <td>82.40</td><td>65.20</td><td>25.36</td><td>31.62</td><td>68.77</td><td>83.19</td><td>97.77</td><td>52.78</td>
                  </tr>
                  <tr class="highlight">
                      <td><b>InternVL3-8B-Instruct (EvoLMM)</b></td>
                      <td><b>84.97</b></td><td><b>67.20</b></td><td><b>26.44</b></td><td><b>32.92</b></td><td><b>69.39</b></td><td><b>83.95</b></td><td><b>98.13</b></td><td><b>53.77</b></td>
                  </tr>
                  <tr>
                      <td>Gemma-3-12B-It (Base)</td>
                      <td>55.64</td><td>60.13</td><td>24.53</td><td>28.96</td><td>50.69</td><td>79.05</td><td>83.89</td><td>48.11</td>
                  </tr>
                  <tr class="highlight">
                      <td><b>Gemma-3-12B-It (EvoLMM)</b></td>
                      <td><b>58.61</b></td><td><b>62.13</b></td><td><b>25.61</b></td><td><b>30.26</b></td><td><b>51.37</b></td><td><b>79.85</b></td><td><b>84.97</b></td><td><b>49.10</b></td>
                  </tr>
                  <tr>
                      <td>Llama-3.2-11B-Vision-Instruct (Base)</td>
                      <td>29.24</td><td>46.59</td><td>23.47</td><td>37.23</td><td>56.69</td><td>46.44</td><td>56.87</td><td>47.93</td>
                  </tr>
                  <tr class="highlight">
                      <td><b>Llama-3.2-11B-Vision-Instruct (EvoLMM)</b></td>
                      <td><b>32.24</b></td><td><b>48.59</b></td><td><b>24.55</b></td><td><b>38.53</b></td><td><b>57.37</b></td><td><b>47.32</b></td><td><b>58.07</b></td><td><b>48.92</b></td>
                  </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>

    </div>
  </section>


  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>
@article{thawakar2025evolmm,
  title={EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards},
  author={Thawakar, Omkar and Venkatraman, Shravan and Thawkar, Ritesh and Shaker, Abdelrahman M and Cholakkal, Hisham and Anwer, Rao Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv},
  year={2025}
}
    </code></pre>
  </div>
</section>


  <!-- Acknowledgement -->
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a
        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        We are thankful to open-source LMM projects for releasing models/code and templates.
      </p>
    </div>
  </section>

  <div style="text-align: center;">
    <a href="https://www.ival-mbzuai.com/" target="_blank">
      <img src="./assets/IVAL_logo.png" width="200" height="100" alt="IVAL Logo">
    </a>
    <a href="https://github.com/mbzuai-oryx" target="_blank">
      <img src="./assets/Oryx_logo.png" width="100" height="100" alt="Oryx Logo">
    </a>
    <a href="https://mbzuai.ac.ae/" target="_blank">
      <img src="./assets/MBZUAI_logo.png" width="360" height="85" alt="MBZUAI Logo">
    </a>
  </div>

</body>
</html>
